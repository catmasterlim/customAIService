{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 흐름\n",
    "\n",
    "- 모델 선택 -> download -> ollama에 로드 -> 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 fine tunning model \n",
    "\n",
    "아래 야놀자에서 만든 한글로 파인튜닝된 모델을 다운로드\n",
    "https://huggingface.co/\n",
    "https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0\n",
    "https://huggingface.co/heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 선택 후 다운로드\n",
    "- 사이트를 참조\n",
    "  -  https://huggingface.co/\n",
    "\n",
    "#### huggingface \n",
    "- 모델 선택 후 User this model에서 ollama를 선택\n",
    "> ex ) ollama run hf.co/heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF:Q5_K_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 gguf로 변환(양자화)\n",
    "\n",
    "- https://github.com/abetlen/llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install huggingface_hub\n",
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# download model\n",
    "model_name_or_path = \"heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF\" # repo id\n",
    "# \n",
    "model_basename = \"ggml-model-Q5_K_M.gguf\" # file name\n",
    "model_name = \"EEVE-Korean-Q5_K_M\"\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=model_name_or_path\n",
    "    , filename=model_basename\n",
    "    )\n",
    "print(f\" model_name : {model_name}\")\n",
    "print(f\" model_path : {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CPU\n",
    "# lcpp_llm = Llama(\n",
    "#     model_path=model_path,\n",
    "#     n_threads=2,\n",
    "#     )\n",
    "\n",
    "# GPU에서 사용하려면 아래 코드로 실행\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2, # CPU cores\n",
    "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=43, # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_ctx=4096, # Context window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: {prompt}\\nAssistant:\\n\"\n",
    "text = '한국의 수도는 어디인가요?'\n",
    "\n",
    "prompt = prompt_template.format(prompt=text)\n",
    "\n",
    "start = time.time()\n",
    "response = lcpp_llm(\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    stop = ['</s>'], # Dynamic stopping when such token is detected.\n",
    "    echo=True # return the prompt\n",
    ")\n",
    "pprint(response)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.delete(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load to ollama \n",
    "import ollama\n",
    "\n",
    "modelfile=f'''\n",
    "FROM \"{model_path}\"\n",
    "\n",
    "SYSTEM You are \"Knd도움이\".\n",
    "'''\n",
    "\n",
    "ollama.create(model=model_name, modelfile=modelfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "for model in ollama.list().models:\n",
    "    print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘이 파란 이유는 공기 속에 들어 있는 수증기나 먼지 같은 미세한 입자들 때문입니다. 이 작은 입자들은 햇빛, 특히 푸른색 빛에 대해 산란을 일으킵니다. 하늘의 색깔은 실제로 우리가 볼 때 다양한 색들이 혼합된 것입니다만, 그 안에는 주로 녹색과 파란색이 포함되어 있고 조금의 보라색도 있습니다.\n",
      "\n",
      "우리가 하늘을 올려다볼 때 보이는 모든 색들을 분해하면, 가장 많이 기여하는 색은 파란색입니다. 이러한 산란 현상이 바로 우리가 하늘에서 파란색을 보는 이유이며, 이는 지구 대기권에 존재하고 있는 수증기나 먼지 같은 입자들 때문입니다.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "model_name = \"EEVE-Korean-Q5_K_M\"\n",
    "\n",
    "client = Client(\n",
    "  host='http://localhost:11434',\n",
    "  headers={'x-some-header': 'some-value'}\n",
    ")\n",
    "response = client.chat(model=model_name, stream=True, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': '왜 하늘은 파래?',\n",
    "  },\n",
    "])\n",
    "\n",
    "resultText = ''\n",
    "for res in response:\n",
    "  resultText += res.message.content\n",
    "  # print()   \n",
    "\n",
    "print(resultText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
