{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 흐름\n",
    "\n",
    "- 모델 선택 -> download -> ollama에 로드 -> 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 fine tunning model \n",
    "\n",
    "아래 야놀자에서 만든 한글로 파인튜닝된 모델을 다운로드\n",
    "https://huggingface.co/\n",
    "https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0\n",
    "https://huggingface.co/heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 선택 후 다운로드\n",
    "- 사이트를 참조\n",
    "  -  https://huggingface.co/\n",
    "\n",
    "#### huggingface \n",
    "- 모델 선택 후 User this model에서 ollama를 선택\n",
    "> ex ) ollama run hf.co/heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF:Q5_K_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 gguf로 변환(양자화)\n",
    "\n",
    "- https://github.com/abetlen/llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install huggingface_hub\n",
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# download model\n",
    "model_name_or_path = \"heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF\" # repo id\n",
    "# \n",
    "model_basename = \"ggml-model-Q5_K_M.gguf\" # file name\n",
    "model_name = \"EEVE-Korean-Q5_K_M\"\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=model_name_or_path\n",
    "    , filename=model_basename\n",
    "    )\n",
    "print(f\" model_name : {model_name}\")\n",
    "print(f\" model_path : {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CPU\n",
    "# lcpp_llm = Llama(\n",
    "#     model_path=model_path,\n",
    "#     n_threads=2,\n",
    "#     )\n",
    "\n",
    "# GPU에서 사용하려면 아래 코드로 실행\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2, # CPU cores\n",
    "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=43, # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_ctx=4096, # Context window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: {prompt}\\nAssistant:\\n\"\n",
    "text = '한국의 수도는 어디인가요?'\n",
    "\n",
    "prompt = prompt_template.format(prompt=text)\n",
    "\n",
    "start = time.time()\n",
    "response = lcpp_llm(\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    stop = ['</s>'], # Dynamic stopping when such token is detected.\n",
    "    echo=True # return the prompt\n",
    ")\n",
    "pprint(response)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.delete(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load to ollama \n",
    "import ollama\n",
    "\n",
    "modelfile=f'''\n",
    "FROM \"{model_path}\"\n",
    "\n",
    "SYSTEM You are \"Knd도움이\".\n",
    "'''\n",
    "\n",
    "ollama.create(model=model_name, modelfile=modelfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "for model in ollama.list().models:\n",
    "    print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```css\n",
      "#svg-container {\n",
      "  width: 100%;\n",
      "  height: 500px;\n",
      "}\n",
      "\n",
      "svg {\n",
      "  width: 100%;\n",
      "  height: 100%;\n",
      "}\n",
      "\n",
      "path.cloud {\n",
      "  fill: #d9dffa;\n",
      "  shape-rendering: auto;\n",
      "}\n",
      "\n",
      "path.mountain {\n",
      "  fill: #7f8c8d;\n",
      "  shape-rendering: auto;\n",
      "}\n",
      "\n",
      "circle.sun {\n",
      "  fill: #fba456;\n",
      "  stroke: none;\n",
      "  r: 20%;\n",
      "}\n",
      "\n",
      "polygon.dragonball {\n",
      "  fill: url(#dragonball_pattern);\n",
      "  shape-rendering: auto;\n",
      "}\n",
      "\n",
      "circle.cloud_particle {\n",
      "  fill: #f8f9fa;\n",
      "  stroke: none;\n",
      "  r: 5px;\n",
      "}\n",
      "\n",
      "rect.son_goku {\n",
      "  fill: url(#dragonball_pattern);\n",
      "  shape-rendering: auto;\n",
      "}\n",
      "\n",
      "path.cloud_arc {\n",
      "  fill: #d9dffa;\n",
      "  stroke: none;\n",
      "  opacity: 0.4;\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "model_name = \"EEVE-Korean-Q5_K_M\"\n",
    "\n",
    "client = Client(\n",
    "  host='http://localhost:11434',\n",
    "  headers={'x-some-header': 'some-value'}\n",
    ")\n",
    "response = client.chat(model=model_name, stream=True, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': '''\n",
    "    노을이 지는 그림에 드래곤볼 손오공 나오는 그림 그려줘. \n",
    "    - 그림 포멧 : svg\n",
    "    - 결과를 스스로 재평가\n",
    "    ''',\n",
    "  },\n",
    "])\n",
    "\n",
    "resultText = ''\n",
    "for res in response:\n",
    "  resultText += res.message.content\n",
    "  # print()   \n",
    "\n",
    "print(resultText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
